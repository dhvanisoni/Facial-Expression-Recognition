# Facial-Expression-Recognition 

## About Dataset:
The Fer2013 dataset, obtained from a Kaggle facial expression recognition competition known as the Facial Expression Recognition Challenge, has been selectively curated for our study. The FER-2013 dataset is used for this study, which contains 35,887 grayscale facial images of different expressions with size restricted to 48x48 pixels. Each image is labeled with one of seven emotion categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). Within the scope of our investigation, we have specifically narrowed down the dataset to encompass three distinct emotional categories: 'happy,' 'sad,' and 'neutral’ with training images 9080 and testing images 2280. The training data contains3020 neutral images, 3010 happy images and 3150 sad images. It is noteworthy that this dataset is characterized by its inclusivity, encompassing faces spanning diverse age ranges and orientations. Remarkably, our focused investigation demonstrates the efficacy of facial expression recognition, particularly within the context of the 'happy,' 'sad,' and 'neutral' emotional classifications. A visual depiction of sample images from the Fer2013 dataset is presented in Figure 1 for reference. 

## Methodology

### CNN With Batch Normalization:
The methodology employed in this project involves the utilization of a Convolutional Neural Network (CNN) architecture for the task of facial expression recognition. The chosen CNN model is constructed using the Keras library, comprising multiple layers such as convolutional, batch normalization, activation, max-pooling, dropout, and fully connected layers. the CNN architecture initially needs to extract input pictures of 48x48x1 (48 wide, 48 high, 1 color channel) from dataset FER-2013. The network starts with an input layer equal to the input data dimension of 48×48. The model contains three convolutional blocks. Each block consists of a convolutional layer with a specific number of filters and 3×3 kernel size. After each convolutional layer, we apply batch normalization and the ReLU activation function to introduce non-linearity. Max pooling with a pool size of (2,2) is applied to reduce spatial dimensions, followed by dropout with a rate of 0.25 to mitigate overfitting. After flattening the output from the convolutional layers, we include two fully connected blocks. Each block comprises a dense layer with a specified number of units, batch normalization, ReLU activation, and dropout with a rate of 0.25. The last layer is a dense layer with 3 units, corresponding to the number of facial expression classes. The softmax activation function is used to obtain probability distributions over the classes. The key parameters, including image size (img_size) and batch size (batch_size), are configured to facilitate effective data preprocessing and model training. Data augmentation is applied to both the training and validation datasets using the ImageDataGenerator to enhance the model's generalization capabilities. The CNN is compiled with the Adam optimizer, employing a learning rate of 0.0005, and is optimized to minimize categorical cross-entropy loss. The training process spans multiple epochs, and model performance is continuously monitored using checkpointing and learning rate reduction callbacks. Ultimately, the methodology integrates these components harmoniously, enabling the CNN to learn and recognize intricate patterns within facial expressions and achieve notable accuracy in predicting emotion categories.
