# Facial-Expression-Recognition 

## About Dataset:
The Fer2013 dataset, obtained from a Kaggle facial expression recognition competition known as the Facial Expression Recognition Challenge, has been selectively curated for our study. The FER-2013 dataset is used for this study, which contains 35,887 grayscale facial images of different expressions with size restricted to 48x48 pixels. Each image is labeled with one of seven emotion categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). Within the scope of our investigation, we have specifically narrowed down the dataset to encompass three distinct emotional categories: 'happy,' 'sad,' and 'neutral’ with training images 9080 and testing images 2280. The training data contains3020 neutral images, 3010 happy images and 3150 sad images. It is noteworthy that this dataset is characterized by its inclusivity, encompassing faces spanning diverse age ranges and orientations. Remarkably, our focused investigation demonstrates the efficacy of facial expression recognition, particularly within the context of the 'happy,' 'sad,' and 'neutral' emotional classifications. A visual depiction of sample images from the Fer2013 dataset is presented in Figure 1 for reference. 

## Methodology

### 1. CNN With Batch Normalization:
The methodology employed in this project involves the utilization of a Convolutional Neural Network (CNN) architecture for the task of facial expression recognition. The chosen CNN model is constructed using the Keras library, comprising multiple layers such as convolutional, batch normalization, activation, max-pooling, dropout, and fully connected layers. the CNN architecture initially needs to extract input pictures of 48x48x1 (48 wide, 48 high, 1 color channel) from dataset FER-2013. The network starts with an input layer equal to the input data dimension of 48×48. The model contains three convolutional blocks. Each block consists of a convolutional layer with a specific number of filters and 3×3 kernel size. After each convolutional layer, we apply batch normalization and the ReLU activation function to introduce non-linearity. Max pooling with a pool size of (2,2) is applied to reduce spatial dimensions, followed by dropout with a rate of 0.25 to mitigate overfitting. After flattening the output from the convolutional layers, we include two fully connected blocks. Each block comprises a dense layer with a specified number of units, batch normalization, ReLU activation, and dropout with a rate of 0.25. The last layer is a dense layer with 3 units, corresponding to the number of facial expression classes. The softmax activation function is used to obtain probability distributions over the classes. The key parameters, including image size (img_size) and batch size (batch_size), are configured to facilitate effective data preprocessing and model training. Data augmentation is applied to both the training and validation datasets using the ImageDataGenerator to enhance the model's generalization capabilities. The CNN is compiled with the Adam optimizer, employing a learning rate of 0.0005, and is optimized to minimize categorical cross-entropy loss. The training process spans multiple epochs, and model performance is continuously monitored using checkpointing and learning rate reduction callbacks. Ultimately, the methodology integrates these components harmoniously, enabling the CNN to learn and recognize intricate patterns within facial expressions and achieve notable accuracy in predicting emotion categories.

### 2. Transfer Learning with VGGNet-16:
The second approach of this study is to develop a facial expression classification model using transfer learning with VGGNet-16. Specifically, we aim to classify facial expressions into three emotion categories: happy, sad, and neutral. We preprocess the FER-2013 dataset by normalizing pixel values to the range [0, 1]. We leverage the pre-trained VGGNet-16 model, which was trained on ImageNet, as a feature extractor. We remove the fully connected layers from VGGNet-16 and retain only the convolutional layers. To preserve their pre-trained weights, we freeze these convolutional layers, ensuring that they remain unchanged during our training process. For the final layer of the model, we design a custom classifier tailored to our specific facial expression classification task. The custom classifier replaces the original fully connected layers of VGGNet-16. It consists of two fully connected layers with 4096 nodes in the first layer and 3 nodes in the last layer. The first layer captures more abstract representations from the convolutional layers, while the last layer has 3 nodes to correspond to our three facial expression classes: "happy," "sad," and "neutral." By freezing all layers except the last one, we focus the training process on fine-tuning the model's output layer to suit our specific classification task. This approach allows us to take advantage of the powerful feature extraction capabilities of VGGNet-16 while tailoring the model's classification abilities to recognize facial expressions accurately. We employ the SGD optimizer with a learning rate of 0.001 and momentum of 0.9 for training the model. The model is trained for different number of epochs 10, 15, 25 using a batch size of 256. We use the categorical cross-entropy loss function as it is appropriate for multi-class classification tasks. For model evaluation, we use accuracy as the primary metric, which measures the percentage of correctly classified samples in the test set. Additionally, we present the confusion matrix to visualize the classification performance for each emotion category.

### 3. Transfer Learning with ResNet-18:
In this study, we adopt another pre trained ResNet-18 model for transfer learning in image classification. The ResNet architecture is a deep convolutional neural network known for its ability to handle the vanishing gradient problem, enabling training of much deeper networks. The model consists of multiple layers, including convolutional layers, batch normalization layers, ReLU activation functions, and residual blocks.The first layer is a 7x7 convolutional layer with a stride of 2, followed by batch normalization and ReLU activation. Subsequently, a max-pooling layer is applied. The model consists of four sequential blocks (layer1 to layer4), each containing two BasicBlock modules. Each BasicBlock includes two 3x3 convolutional layers with batch normalization and ReLU activation. The first convolutional layer preserves the input dimensions, while the second one downsamples the feature maps spatially by a stride of 2 if needed. A downsample operation is performed in each BasicBlock when the spatial dimensions change, ensuring compatibility with the skip connections.The AdaptiveAvgPool2d layer is utilized to average the spatial dimensions of the feature maps to a size of (1,1), and the fully connected (fc) layer is applied, with 512 input features and 1000 output features. The fc layer was originally designed for 1000 classes in the ImageNet dataset; however, we modify it for our specific image classification task with three classes: "happy," "sad," and "neutral." For transfer learning, we remove the last fully connected layer designed for the ImageNet task, as we aim to classify facial expressions into three categories. We retain all the convolutional layers in the ResNet model and freeze them to retain their pre-trained weights. A custom classifier is then designed with two fully connected layers with 4096 and 3 nodes, respectively. The first layer captures abstract representations from the convolutional layers, while the last layer outputs the final classification probabilities for the three facial expression classes. During training, we use the stochastic gradient descent (SGD) optimizer with a learning rate of 0.001 and a momentum of 0.9. The model is trained for different numbers of epochs, ranging from 10, 15, to 25, using a batch size of 256. We employ the categorical cross-entropy loss function, which is suitable for multi-class classification tasks. The primary evaluation metric for model performance is accuracy, which measures the percentage of correctly classified samples in the test set.By utilizing the pre-trained ResNet model as a feature extractor and fine-tuning the custom classifier for facial expression classification, we aim to take advantage of the powerful feature representation capabilities of ResNet while adapting the model for our specific task. 
